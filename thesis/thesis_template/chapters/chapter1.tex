\chapter{Introduction}
\label{chap:intro}
\chaptermark{}

\section{Early Approaches to Automation in the Legal Sphere}
\label{sec:early}

The application of Artificial Intelligence in the legal domain began in 1958 with Lucien Mehl's seminal article \cite{automation_in_legal_1958}, which explored AI's potential in law and decision-making processes. Mehl proposed two primary approaches for legal AI systems:
\begin{itemize}
    \item \textit{Information machine} - "the machine for finding precedent"
    \item \textit{Consultation machine} - "the judgement machine"
\end{itemize}

The author argued that both approaches required transforming natural language in legal documents into a strictly formalized, machine-readable format. This task was considered a preliminary step toward formalizing natural language as a whole. While Mehl proposed a framework for legal text formalization, this remains an unsolved challenge today.\footnote{
    Modern projects like \cite{merigouxCatalaProgrammingLanguage2021} continue to explore machine-readable formats for legislative texts.
}

Early probabilistic approaches to natural language processing emerged in 1952 \cite{weaverTranslation1952}, but were deemed impractical due to computational constraints. Only in 1990 did IBM introduce efficient, computationally feasible methods \cite{brownStatisticalApproachMachine1990}. A 2003 review \cite{risslandAILawFruitful2003} confirmed that case-based and rule-based reasoning systems remained the most prevalent AI applications in law at that time.

\section{Large Language Models in Law}
Advances in computational power enabled complex natural language processing models. The field transformed with Word2Vec's semantically meaningful word vectors (2013) \cite{mikolovEfficientEstimationWord2013}, followed by Google's Transformer architecture (2017) \cite{vaswaniAttentionAllYou2017}. OpenAI's 2020 paper \cite{brownLanguageModelsAre2020} introduced few-shot learning through prompting, leading to powerful, accessible AI assistants applicable to legal domains.

A 2024 Texas survey \cite{seabrookeSurveyLayPeoples2024} revealed that over 50\% of respondents would consult AI assistants for tenancy, tax, and traffic legal matters.\footnote{
    However, only $\approx$30\% trusted AI for divorce, juvenile, or civil dispute cases.
} Similarly, LexisNexis \cite{LawyersGearGenerative} reported 35\% of lawyers using AI assistants monthly.

Specialized legal LLMs like LawGPT (2023) \cite{nguyenBriefReportLawGPT2023}, LawyerLLaMA (2023) \cite{huangLawyerLLaMATechnical2023}, and ChatLaw (2023) \cite{cuiChatlawMultiAgentCollaborative2024a} have emerged. Despite LLMs' strong performance in general domains, legal applications face accuracy challenges. Stanford research (2024) \cite{mageshHallucinationFreeAssessingReliability2024a} shows fine-tuned legal LLMs still hallucinate in 17-33\% of cases. OpenAI's theoretical analysis \cite{kalaiCalibratedLanguageModels2024} confirms all language models retain measurable hallucination probabilities regardless of size or training.

Legal domains present unique challenges:
\begin{itemize}
    \item \textbf{Continuous evolution}: Legislative acts constantly expand and change
    \item \textbf{Outdated information}: Documents may be deprecated, replaced, or amended
    \item \textbf{Jurisdictional variation}: Legal documents apply differently across regions, requiring localized fine-tuning
\end{itemize}

These factors heighten risks when using hallucination-prone AI for felony or misdemeanor cases, particularly when relying solely on "consultation machines" without verified document retrieval.

\section{RAG Approach}
Retrieval-Augmented Generation (RAG), introduced in 2020 \cite{lewisRetrievalAugmentedGenerationKnowledgeIntensive2020}, combines LLMs with reliable information retrieval systems (see Section 2). \cite{mageshHallucinationFreeAssessingReliability2024a} demonstrates RAG can reduce legal QA hallucinations to near-zero with proper context retrieval.

While RAG-based legal QA systems are rapidly developing (detailed in Section 2), existing research focuses on Western (primarily US and Australian) legal systems. Though Russian applications exist (e.g., \cite{kolaScienceCenter}), none address legal domains. Our work applies and evaluates RAG in Russian legal contexts, providing foundational insights for future research.
% This is a section. This is a citation without brackets.
% and this is one with brackets \cite{A}. Multiple \cite{A,B,C}
%  Here's a reference to a subsection: \ref{sec:subsection}. 
%  Citation of an online article \cite{D}. Citation of an online proceeding \cite{F}. 
% The body of the text and abstract must be double-spaced except for footnotes or long quotations. 
%  Fonts such as Times Roman, Bookman, New Century Schoolbook, Garamond, Palatine, 
%  and Courier are acceptable and commonly found on most computers. 
%  The same type must be used throughout the body of the text. The font size must be 10 point or 
%  larger and footnotes\footnote{This is a footnote.} must be two sizes smaller 
%  than the text\footnote{This is another footnote.} but no smaller than eight points. 
%  Chapter, section, or other headings should be of a consistent font and size throughout the ETD, 
%  as should labels for illustrations, charts, and figures.


\subsection{Creating a Subsection}
\label{sec:subsection}

\subsubsection{Creating a Subsubsection}
\subsubsection{Creating a Subsubsection}
\subsubsection{Creating a Subsubsection}

\paragraph{This is a heading level below subsubsection}

And this is a quote: 
%
\begin{quote}
\blindtext
\end{quote}

\begin{figure}[hbt]
\centering
\includegraphics[]{figs/images.png}
\caption{One kernel at $x_s$ (\emph{dotted kernel}) or two kernels at
$x_i$ and $x_j$ (\textit{left and right}) lead to the same summed estimate
at $x_s$. This shows a figure consisting of different types of
lines. Elements of the figure described in the caption should be set in
italics, in parentheses, as shown in this sample caption.}
\label{fig:example}
\end{figure}

This is a table:
% currsize is not set in the long table environment, so we need to set it before we set it up.
\makeatletter
\let\@currsize\normalsize
\makeatother

% tabular environments are set to be single-spaced in the  thesis class,  but long tables do not use tabular
% to get around this, set the spacing to single spacing at the start of the long table environment, and set it back to double-spacing at the end of it

\begin{longtable}{c|c|c}
\caption[This is the title I want to appear in the List of Tables]{This Is a Table Example} \label{tab:pfams} \\
\hline
A & B & C \\
\hline
\endfirsthead
\multicolumn{3}{@{}l}{} \\
\hline
A & B & C\\
\hline
\endhead
a1 & b1 & c1 \\
a2 & b2 & c2\\
a3 & b3 & c3\\
a4 & b4 & c4\\
\hline
\end{longtable}

The package ``upgreek'' allows us to use non-italicized lower-case greek letters. See for yourself: $\upbeta$, $\bm\upbeta$, $\beta$, $\bm\beta$. Next is a numbered equation:
\begin{align}
\label{eq:name}
\|\bm{X}\|_{2,1}={\underbrace{\sum_{j=1}^nf_j(\bm{X})}_{\text{convex}}}=\sum_{j=1}^n\|\bm{X}_{.,j}\|_2
\end{align}
The reference to equation (\ref{eq:name}) is clickable. 
\section[Theorems, Corollaries, Lemmas, Proofs, Remarks, Definitions and Examples]{Theorems, Corollaries, Lemmas, Proofs, Remarks, Definitions,and Examples}

\begin{theorem}
\label{thm:onlytheorem}
\blindtext
\end{theorem}

\begin{proof}
I'm a (very short) proof.
\end{proof}

\begin{lemma}
I'm a lemma.
\end{lemma}

\begin{corollary}
I include a reference to Thm. \ref{thm:onlytheorem}.
\end{corollary}

\begin{proposition}
I'm a proposition.
\end{proposition}

\begin{remark}
I'm a remark. 
\end{remark}

\begin{definition}
I'm a definition. I'm a definition. I'm a definition. I'm a definition. I'm a definition. I'm a definition. I'm a definition. I'm a definition. I'm a definition. I'm a definition. I'm a definition. 
\end{definition}

\begin{example}
I'm an example.
\end{example}


\section[Optional table of contents heading]{Section with\\ linebreaks in\\the
name}


\Blindtext[2]




